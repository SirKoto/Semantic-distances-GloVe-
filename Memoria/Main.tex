\documentclass[catalan,10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[catalan]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{listings}
\lstset{language=C++}
\usepackage{fancyhdr}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\pagestyle{fancy}
\usepackage{tikz}
\usepackage{nameref}
\usepackage[hidelinks,colorlinks=true, linkcolor=blue,citecolor={blue},urlcolor=blue]{hyperref}
\lhead{Projecte TGA: \textit{word2vec}}
\rhead{Antoni Casas Muñoz\\Pol Martín Garcia}

\begin{document}
	\begin{titlepage}
		\centering
		{\bfseries\LARGE Universitat Politècnica de Catalunya \par}
		\vspace{1cm}
		{\scshape\Large Facultat d'Informàtica de Barcelona\par}
		\vspace{3cm}
		{\scshape\Huge Distàncies \textit{word2vec} \par}
		\vspace{3cm}
		{\itshape\Large Projecte Targetes Gràfiques (TGA) \par}
		\vfill
		{\Large Antoni Casas Muñoz \par}
		{\Large Pol Martín Garcia \par}
		\vfill
		{\Large Maig 2020 \par}
	\end{titlepage}
	
	\newpage
	
\section*{Problema a resoldre}

El problema a solucionar és la computació de similitud de paraules utilitzant el model \textit{word2vec} de \textit{GloVe} \cite{GloVeGlo18:online} amb la mètrica de similitud de cosinus \cite{Cosinesi72:online}.

\textit{Word2Vec} és una representació densa d'una paraula en un espai vectorial reduït, on la representació manté les analogies semàntiques amb operacions aritmètiques simples, com per exemple $\text{water} + \text{freeze} \simeq \text{ice}$, o $\text{king} - \text{man} + \text{woman} \simeq \text{queen}$. Això permet operar amb paraules, específicament amb els seus significats, igual que es podria operar amb altres tipus de dades, permetent l'ús d'aquestes representacions per a múltiples tasques.

D'entre totes les operacions que es poden realitzar en un espai vectorial, una de les més senzilles i útils de computar és la distància de dos o més punts en l'espai, així com la cerca dels punts més propers a un altre punt concret d'entre les dades. D'aquesta manera, l'objectiu d'aquest treball és usar aquesta propietat per a poder computar distàncies semàntiques entre paraules, i cercar paraules per proximitat semàntica usant la representació vectorial d'aquestes.

Mentre que per a convertir una paraula a la seva representació densa és tan simple com accedir a un diccionari i extreure el valor, sent un procés només intensiu en espai un cop el diccionari ja ha estat generat, presenten una complicació al fer la conversió de la representació densa (\textit{word2vec}) a la representació esparsa (el vocabulari de l'idioma en què \textit{word2vec} va ser entrenat) aquesta conversió és especialment difícil si s'han dut a terme operacions aritmètiques amb aquesta representació.\newline
La manera de realitzar aquesta conversió és trobar la paraula, o paraules, més properes, i per això s'utilitza la similitud de cosinus. No s'empra la distància euclidiana típica, ja que aquesta mètrica ofereix poc significat en espais amb un gran nombre de dimensions, com és el cas de molts models \textit{word2vec}. En el nostre cas l'espai és de 300 dimensions, és a dir que cada paraula té una representació densa corresponent a un vector de 300 components, també anomenat \textit{embedding}, i per tant la distància euclidiana no seria una mètrica vàlida.

La similitud de cosinus \cite{Cosinesi72:online} és una mètrica de similitud que és utilitzada per ser fàcil de computar, i oferir valors en el rang de $[-1,1]$, essent 1 el valor que indica absoluta similitud Aquesta mètrica es computa com amb la següent equació \ref{eq:cosSim}. La mètrica no mesura la distància en l'espai, sinó que mesura com són de paral·leles les representacions.
\begin{equation} \label{eq:cosSim}
	\text{cosSim}(\vec A,\vec B) = \frac{\vec A\bullet \vec B}{|\vec A|\cdot|\vec B|}
\end{equation}

\subsubsection*{Canvis al model}
Per a la implementació de l'algoritme, primer hem obtingut el model de \textit{word2vec} de \textit{GloVe} \cite{GloVeGlo18:online}, i l'hem modificat ordenant alfabèticament les paraules i afegint les normes de cada representació densa al model propi, d'aquesta manera no és necessari computar la norma en cada operació de similitud. També, s'ha fet una millora en implementacions consecutives, on el model són dos fitxers, un amb les paraules ordenades, i un altre fitxer binari amb les representacions denses emmagatzemades. El model llavors està format per unes $2.2\cdot 10^{6}$ paraules (concretament $2196016$ paraules), on cada paraula és una línia al fitxer, primer la paraula, després la norma, i finalment els 300 valors de la representació densa.	

És adient mencionar que cadascuna de les components de les representacions denses de les paraules es troba en el rang $[-1,1]$, d'aquesta manera s'obté una millora de precisió en emmagatzemar i operar amb nombres de coma flotant.

\section*{Ús del software}
Per poder compilar el codi font s'ha creat un sistema de \verb|CMake|. D'aquesta manera es pot compilar el programa independentment de la plataforma.

És necessari, per al correcte funcionament de la configuració del \textit{makefile}, que \verb|CMake| tingui la referencia del compilador de CUDA prèviament (usualment en un arxiu \verb|CMakeCUDACompiler.cmake|); sinó s'haurà d'emmagatzemar a l'entrada \verb|CMAKE_CUDA_COMPILER| el \textit{path} del compilador de CUDA.

Actualment el projecte està configurat per a usar memòria \textit{pinned} per defecte. De no voler emmagatzemar les dades en aquest tipus de memòria cal modificar l'arxiu \verb|CMakeLists.txt| i des-comentar la línia 7, que activa la definició de compilació \verb|NOT_PINNED_MEMORY|. \newline
Per altra banda, el projecte també està configurat per generar codi per arquitectura Turing (\textit{sm\_75}). De voler generar codi per a una altra arquitectura cal modificar la línia 75 de l'arxiu \verb|CMakeLists.txt| adientment.

Un cop el projecte ja es troba compilat, hi ha dues maneres d'executar el programa resultant.
\begin{itemize} % ELS LINKS ESTAN CAIGUTS
	\item Dades en arxiu TXT. Passar un sol paràmetre, corresponent amb el \textit{path} d'un arxiu de text amb les paraules, normes i \textit{embeddings}. El fitxer usat per nosaltres es pot obtenir de \url{https://workbench.ddns.net/nextcloud/index.php/s/mcxC38NDMMzmDgQ}.
	\item Paraules en TXT, i binari amb dades. Passar dos paràmetres. Un primer \textit{path} a un arxiu amb el llistat de paraules (\textit{keys}), i un segon \textit{path} a un arxiu binari amb les normes i els \textit{embeddings} preprocessats (\textit{values}). Els arxius són respectivament \url{https://workbench.ddns.net/nextcloud/index.php/s/qtkbG6Nxx2wWLnf} i \url{https://workbench.ddns.net/nextcloud/index.php/s/6FAjH6QP6sYzf9c}.
\end{itemize}
La diferència entre les dues entrades és que la segona és notablement més ràpida que la primera, ja que no necessita dur a terme el \textit{parsing} dels nombres en coma flotant.

Amb les dades carregades, s'imprimeixen per consola els temps i altra informació interessant sobre aquesta etapa, i s'inicia l'entrada de dades per a dur a terme computacions. \newline
Aquesta entrada ha de consistir d'una paraula arbitrària i d'un valor ${0,1}$. El valor identifica si s'ha d'executar també el còmput corresponent en CPU.

A partir de la \nameref{sec:v3}, es pot també introduir una operació aritmètica amb sumes i restes entre paraules per buscar paraules amb relacions similars. Per a això s'introdueix l'anomenada operació finalitzada amb el símbol "!``, seguit del valor ${0,1}$ per executar el còmput corresponent a CPU.

Aquí hi ha uns exemples d'entrada:
\begin{itemize}
	\item \verb|bottle 0|: computa les paraules més similars a \textit{bottle} a GPU.
	\item \verb|snake 1|: computa tant a CPU com a GPU.
	\item \verb|king - queen ! 0|: computa les paraules que tenen una relació similar a l'operació anterior, a GPU.
\end{itemize}

Per cada terme introduït, és dura a terme una cerca de la paraula introduïda en la base de dades. Si aquesta hi és, es procedirà a computar i escriure per consola les 10 paraules sintàcticament més semblants en la codificació \textit{word2vec}, així com diverses mètriques per avaluar l'eficiència de la consulta.

El programa acaba quan es tanca la \textit{pipe} d'entrada.


\section*{Implementació}
Hi ha dues versions correctament implementades del projecte, amb resultats finals equivalents.

Aquestes versions també són equivalents al codi seqüencial, trobat al fitxer \textit{main.cpp} en la funció \verb|sequentialSearch()|. Aquesta, donat un vector d'\textit{embeddings}, l'índex d'un d'aquests, i un nombre N, troba d'entre tots els \textit{embeddings} del vector els N més semblants a l'\textit{embedding} identificat per l'índex.

\subsection*{Algoritme}
Qualsevol dels algoritmes implementats per a solucionar aquest problema es basen en 3 parts.
\begin{enumerate}
	\item Trobar la paraula en el vector d'\textit{embeddings}. Aquest pas sempre es du a terme amb una cerca binària en CPU, per tant no el discutirem en aquest document.
	\item Dur a terme el còmput de les distàncies de cosinus (o similituds de cosinus) entre tots els \textit{embeddings} i l'\textit{embedding} de la paraula cercada.
	\item Filtrar els resultats, i escollir les N paraules més semblants (amb major similitud) a la paraula cercada amb les dades calculades, de manera ordenada.
\end{enumerate}

Els punts 2 i 3, es troben tan implementats per a CPU, a \textit{main.cpp}, com per GPU, a \textit{kernel.cu}.

\subsection*{Càlcul de similituds}
El càlcul de les distàncies o similituds es du a terme a GPU pel kernel \verb|DotProduct()|, el qual calcula el producte escalar amb cadascun dels \textit{embeddings}, i posteriorment en divideix el resultat pel producte de normes, segons l'equació anterior \ref{eq:cosSim}.

Això es du a terme movent el \textit{embeddings} de la paraula cercada a memòria \textit{shared}, i posteriorment l'usen tots els \textit{threads} del bloc per a dur a terme el producte escalar amb un altre \textit{embedding}, a més es pot fer ús de sincronització dins dels \textit{warps} per obtenir una memòria de menor latència (Vegeu millora de la \nameref{sec:v3}).

D'aquesta manera, cada \textit{thread} computa una sola similitud i inicialitza en el vector de posicions la relació entre la distància i la paraula en el vector original.

\subsection*{Filtrat i ordenació}
El filtrat per GPU requereix conèixer les $N+1$ paraules amb una major similitud, $N+1$ ja que una paraula sempre serà la seva paraula més semblant. Per a això s'ha dividit el còmput d'aquest procés en dues funcions.

La primera, \verb|FirstMerge()|, divideix el vector de similituds resultants en trossos de $N+1$ elements, els quals són ordenats usant ordenació per inserció donat que N+1 sempre serà un nombre petit. L'ordenació es du a terme \textit{on-place}, de manera que es reutilitza la mateixa memòria per emmagatzemar el resultat.\newline
D'aquesta forma, obtenim el vector de similituds en trossos de $N+1$ elements internament ordenats. Evidentment, a part d'aquest vector de similituds s'emmagatzema un vector d'índexs a les paraules originals, per no perdre la relació entre valor de similitud i la respectiva paraula.

Finalment la funció \verb|BotchedMergeSort()| aprofita els segments ordenats per a dur a terme l'ordenació en una reducció del problema. Cada \textit{thread} compara dos dels pedaços de $N+1$ elements prèviament ordenats en un de sol.\newline
Aquesta funció redueix el nombre de similituds a comparar a la meitat per cada crida, i es va usant fins que només resta un sol vector de N elements, el qual identifica les N paraules amb major similitud.

\subsection*{Canvis de versió}
Aquí es llisten les característiques i canvis de cada versió entregada del codi.
\subsubsection*{Primera versió}
La primera versió inclou la implementació més senzilla funcional de l'algoritme. Aquesta permet trobar les 10 paraules sintàcticament més properes a una única paraula introduïda.

Les implementacions dels kernels i càrrega de dades no inclouen cap optimització, per tant aquesta versió és la menys eficient però la base del projecte.

Inclou comparativa d'execució entre algoritme en CPU i GPU.
\subsubsection*{Segona versió}

Les millores en la segona versió del programa són separables en canvis en el codi del kernel, i en carrega de les dades a memòria.

Pel que fa al kernel, s'ha reduït l'espai de memòria reservat, utilitzant memòria local de cada \textit{thread} per emmagatzemar l'ordenació temporal en el mètode \verb|BotchedMergeSort()|. A més, s'ha afegit control d'errors complet.

Per altra banda, s'ha millorat substancialment la càrrega a memòria separant l'arxiu d'input en dos, un que conté els \textit{strings} de les paraules, i un altre que conté les normes i els \textit{embeddings} ja en binari, per estalviar la conversió a float en temps d'execució, a més que els fitxers són de menor mida en binari. Aquesta és la principal optimització d'aquesta versió.

Finalment, s'ha afegit l'opció d'usar o no memòria \textit{pinned} segons una \textit{flag} de compilació.


\subsubsection*{Tercera versió}\label{sec:v3}

La millora en la tercera versió es basa un canvi en el kernel \verb|DotProduct|, perquè com es veurà a la següent secció \nameref{sec:results}, és el coll d'ampolla de l'algoritme. 

En aquest s'han canviat els accessos a memòria per a provocar accessos amb coalescència, fent els accessos consecutius en 32 bytes. Per aquest propòsit, s'han destinat 8 threads per similitud a calcular, ja que els \textit{embeddings} són floats (4 bytes), això provoca accessos contigus en memòria en 32 bytes, que és el límit actual de coalescència d'accessos a memòria en CUDA. Vegeu $\frac{32 \text{ bytes}}{4 \frac{\text{bytes}}{float}} = 8 \text{ floats}$. D'aquesta manera, 8 threads d'un mateix warp s'ocuparan de computar una sola similitud, on cada thread accedeix un valor consecutiu a l'anterior.

S'ha fet ús de shared memory, alocant 4 bytes a cada thread, com a acumulador, reduint els 300 elements d'un embedding en 8, finalment sent reduïts utilitzant una reducció per unrolling que culmina amb un únic thread dels 8 emmagatzemant la distància final a \textit{global memory}.

Cal mencionar que ha estat necessari dur a terme sincronització a nivell de warp, ja que per molt que un warp s'executi sincronitzadament (sempre que no hi haguin divergències), la seva memòria compartida pot no estar-ho. Per a solucionar això s'ha usat \verb|__syncwarp();|.

Per altra banda, aquesta versió inclou una nova funcionalitat, que és el càlcul de similituds amb operacions entre paraules, de manera que es poden trobar altres paraules amb relacions semblants. Això s'ha dut a terme programant un petit \textit{parser} per pila, i posteriorment computant l'operació amb els \textit{embeddings} de les paraules, i usant l'algoritme principal amb l'\textit{embedding} operat. 

\subsubsection*{Quarta versió}

En la quarta versió s'han eliminat \textit{overheads} innecessaris presents al codi, com sincronitzacions explícites, reservar memòria i alliberar-la cada execució si aquesta podia ser reutilitzada. També s'ha editat el format d'output del programa per a diferenciar d'on s'obtenen els diversos temps.

Més important, s'ha millorat altre cop el kernel \verb|DotProduct| lleugerament, eliminant l'ús de memòria compartida per als acumuladors, i usant sincronització entre registres d'un warp usant \verb|__shfl_down_sync()| en la reducció \cite{UsingCUD95:online}, que és més eficient que dur a terme sincronitzacions de memòria compartida.


\section*{Resultats}\label{sec:results}

Els resultats han sigut obtinguts en una màquina amb una GTX 2060 SUPER com a targeta gràfica, utilitzant un slot x16 PCIe 3.0, Intel I5-9600K com a CPU, i els models guardats en una NVME, Samsung 970 EVO, amb suficient RAM DDR4 per a no observar \textit{thrashing}. El sistema no té límits a \textit{pinned memory}, per tant totes les execucions s'han fet utilitzant \textit{pinned memory} i amb CUDA 10.2.


En la implementació seqüencial, com a mitjana de 10 execucions, el temps d'execució d'una cerca és de $2425.9$ms, amb un error estàndard de $7.91$ms, això significa que ha calculat $2196016$ distàncies a una velocitat de $905.2$ distàncies calculades per ms. 

En la primera implementació en CUDA, com a mitjana de 10 execucions, amb una mija de $261.4$ms per execució, amb un error estàndard de $5.19$ms, això indica que ha calculat $2196016$ distàncies a una velocitat de $8400.98$ distàncies per ms. En aquesta implementació, la gran majoria del temps es transferencia de dades, específicament el model de uns 2GB, que cada execució es transferit a la GPU, en total, uns $203$ ms son transferències de Host a Device, $22.6$ms son l'execució del kernel de DotProduct, $1.6$ms el kernel de BotchedMergeSort, i $1$ms el kernel de FirstMerge, la resta sent overheads. La transferència de dades es 10 vegades més significativa que el kernel més costós, per tant, s'ha de treure, ja que les dades son constants.


En la segona implementació de CUDA, com a mitjana de 10 execucions, el temps d'execució d'una cerca és de $36.8$ms, amb un error estàndard de $1.58$ms, això significa que ha calculat $2196016$ distàncies a una velocitat de $59674.3$ distàncies calculades per ms. De els 3 kernels (\verb|DotProduct()|, \verb|FirstMerge()| i \verb|BotchedMergeSort()|), \verb|DotProduct()| ocupa uns 22ms, \verb|FirstMerge()| 0.8ms, i \verb|BotchedMergeSort()| 1.3ms, la resta de temps essent overhead, per tant \verb|FirstMerge()| i \verb|BotchedMergeSort()| es poden considerar negligibles comparat amb \verb|DotProduct()|.

En la tercera implementació de CUDA, com a mitjana de 10 execucions, el temps d'execució d'una cerca és de $17$ms, amb un error estàndard de $0$ms, això significa que ha calculat $2196016$ distàncies a una velocitat de $129177.41$ distàncies calculades per ms. De els 3 kernels (\verb|DotProduct()|, \verb|FirstMerge()| i \verb|BotchedMergeSort()|). \verb|DotProduct()| ocupa uns 6.6ms, \verb|FirstMerge()| 1ms, i \verb|BotchedMergeSort()| 1.6ms. El kernel \verb|DotProduct()| ha tingut una millora significant, gracies als accessos amb coalescència a la memòria.

En la quarta implementació de CUDA, com a mitjana de 10 execucions, el temps d'execució d'una cerca és de $10$ms, amb un error estàndard de $0$ms, això significa que ha calculat $2196016$ distàncies a una velocitat de $219601.6$ distàncies calculades per ms. De els 3 kernels (\verb|DotProduct()|, \verb|FirstMerge()| i \verb|BotchedMergeSort()|). \verb|DotProduct()| ocupa uns 6.5ms, \verb|FirstMerge()| 1ms, i \verb|BotchedMergeSort()| 1.6ms. Es una millora molt lleugera respecta a la tercera implementació respecte els temps de execució dels kernels, la millora en aquest cas es en els overheads reduïts i la reutilització absoluta de tota reserva de memòria a la GPU, no fent free mai.



\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
      & CPU  & GPU 1.0 & GPU 2.0 & GPU 3.0 & GPU 4.0 \\ \hline
ring  & 2405ms & 273ms & 35ms & 17ms & 10ms \\ \hline
key   & 2397ms & 276ms & 46ms & 17ms & 10ms\\ \hline
key   & 2410ms & 240ms & 35ms & 17ms & 10ms\\ \hline
key   & 2403ms & 237ms & 46ms & 17ms & 10ms\\ \hline
ring  & 2436ms & 267ms & 31ms & 17ms & 10ms\\ \hline
king  & 2468ms & 266ms & 35ms & 17ms & 10ms\\ \hline
king  & 2454ms & 275ms & 35ms & 17ms & 10ms\\ \hline
ring  & 2419ms & 276ms & 35ms & 17ms & 10ms\\ \hline
barca & 2414ms & 238ms & 35ms & 17ms & 10ms\\ \hline
messi & 2453ms & 266ms & 35ms & 17ms & 10ms\\ \hline
\end{tabular}
\end{table}




\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
      & Speedup respecte CPU & Speedup respecte anterior & Distàncies per ms \\ \hline
CPU & 1 & 1 & 905.2 \\ \hline
Primera implementació & 9.28 & 9.28 & 8400.98 \\ \hline
Segona implementació & 65.92 & 7.10 & 59674.3  \\ \hline
Tercera implementació & 142.7 & 2.16 & 129177.41  \\ \hline
Quarta implementació & 242.59 & 1.7 & 219601.6 \\ \hline

\end{tabular}
\end{table}

\newpage

\section*{Millores descartades}

Per aprofitar el fet que un fetch de memòria global a cache es de 128 bytes \cite{nvidiaDeveloperDoc}, es va implementar coalescència de memòria als 128 bytes, en comptes dels 32 presents a la ultima versió. Això va donar a resultats uns 5 ms pitjors. Això creiem que es degut a la cache, ja que amb el model de execució actual, tot warp agafa 4 línies de cache i itera sobre elles, provocant 4 hits per línia abans de demanar noves, que permetria una millor utilització de la cache que amb el model de coalescència a 128 bytes.

També es va implementar una diferent sincronització intra-warp, utilitzant una màscara sencera en la primera fase de la reducció i 0 en la resta, però CUDA no afirma que un warp es mantingui convergent fins que s'arribi a una zona divergent en el codi \cite{warpPrimitives}, així que aquesta menor optimització es va treure a favor de posar les masks explícites i no sincronització implícita.

\newpage

\section*{Millores descartades}

\newpage
\bibliographystyle{ieeetr}
\bibliography{biblio}
\end{document}
